# bbrick_oci/otel-collector/tempo-values.yaml
mode: deployment
nameOverride: "backend-portal-oltp"
fullnameOverride: "backend-portal-oltp"

image:
  repository: docker.io/otel/opentelemetry-collector-contrib
  pullPolicy: IfNotPresent
  tag: "0.134.1"
  digest: ""

resources:
  limits:
    cpu: 800m
    memory: 1.5Gi
  requests:
    cpu: 200m
    memory: 400Mi

useGOMEMLIMIT: true

ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    protocol: TCP
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    protocol: TCP
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    protocol: TCP
  healthcheck:
    enabled: true
    containerPort: 13133
    servicePort: 13133
    protocol: TCP

command:
  name: otelcol-contrib

service:
  enabled: true
  type: ClusterIP
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8888"

clusterRole:
  create: true
  rules:
    - apiGroups: [""]
      resources: ["pods", "nodes", "namespaces"]
      verbs: ["get", "list", "watch"]
    - apiGroups: [""]
      resources: ["nodes/metrics", "nodes/stats"]
      verbs: ["get", "list", "watch"]

serviceAccount:
  create: true

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
          max_recv_msg_size_mib: 4
        http:
          endpoint: 0.0.0.0:4318

  processors:
    memory_limiter:
      check_interval: 1s
      limit_percentage: 65
      spike_limit_percentage: 15
    
    batch:
      send_batch_size: 1000
      timeout: 5s
      send_batch_max_size: 2000
    
    # k8sattributes:
    #   auth_type: serviceAccount
    #   passthrough: false
    #   extract:
    #     metadata:
    #       - k8s.pod.name
    #       - k8s.pod.uid
    #       - k8s.deployment.name
    #       - k8s.namespace.name
    #       - k8s.node.name
    #       - k8s.container.name
    
    # probabilistic_sampler:
    #   hash_seed: 22
    #   sampling_percentage: 10
    
    resource:
      attributes:
        - key: cluster
          value: goyo-prod
          action: upsert
    #     - key: service.name
    #       value: goyoai-backend-system-app
    #       action: upsert
    #     - key: service.namespace
    #       value: goyoai-web
    #       action: upsert
    #     - key: deployment.environment
    #       value: prod
    #       action: upsert

  exporters:
    debug:
      verbosity: detailed
    otlp/tempo:
      endpoint: goyo-tempo-distributed-distributor.monitoring.svc.cluster.local:4317
      tls:
        insecure: true
      retry_on_failure:
        enabled: true
        initial_interval: 5s
        max_interval: 60s
        max_elapsed_time: 600s
      sending_queue:
        enabled: true
        num_consumers: 10
        queue_size: 5000
      compression: gzip

  service:
    telemetry:
      metrics:
        address: 0.0.0.0:8888
        level: normal
      logs:
        level: info
        development: false
    
    extensions: [health_check]
    
    pipelines:
      traces:
        receivers: [otlp]
        processors: [memory_limiter, resource, batch]
        exporters: [debug,otlp/tempo]

tolerations:
  - key: "goyo-svc"
    operator: "Equal"
    value: "back"
    effect: "NoSchedule"

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: "goyo-svc"
          operator: In
          values:
          - "back"

serviceMonitor:
  enabled: true

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8888"

livenessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 30
  periodSeconds: 30

readinessProbe:
  httpGet:
    path: /
    port: 13133
  initialDelaySeconds: 10
  periodSeconds: 10

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 65534
  fsGroup: 65534

securityContext:
  runAsNonRoot: true
  runAsUser: 65534
  runAsGroup: 65534
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true
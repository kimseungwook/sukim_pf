---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["namespaces"]
    verbs: ["watch", "list", "get"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "patch", "update"]
  - apiGroups: [""]
    resources:
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes", "csistoragecapacities", "csidrivers"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create","list","watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
        - image: iad.ocir.io/oracle/oci-cluster-autoscaler:1.29.0-10
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            # 로그 상세 레벨 (1-10, 높을수록 더 자세함)
            - --v=6
            # 표준 오류 로그 임계값
            - --stderrthreshold=info
            # 클라우드 제공자 (Oracle Cloud Infrastructure)
            - --cloud-provider=oci
            # 노드 프로비저닝 최대 대기 시간
            - --max-node-provision-time=5m
            # 노드 풀 설정 (min:max:nodepool-id)
            - --nodes=1:5:ocid1.nodepool.oc1.ap-seoul-1.aaaaaaaax5z6enxujzwa5x3xat5vuolvxfql7rgqudx7y4gbino7d7imsylq
            - --nodes=1:5:ocid1.nodepool.oc1.ap-seoul-1.aaaaaaaallcizmybgyodlcs6givo57onixsvx62yi6h2sfwjnntghhvx4fea
            - --nodes=1:5:ocid1.nodepool.oc1.ap-seoul-1.aaaaaaaao7jwshx3ydisfmf566menuuplh432zfbpum55g7gknbua6aohf5a
            - --nodes=1:5:ocid1.nodepool.oc1.ap-seoul-1.aaaaaaaakpnytuf27xli5e3dcatstlsxxojjztrrlw36cfhqgnuoz7vmedya
            # 스케일 다운 활성화
            - --scale-down-enabled=true
            # 노드 추가 후 스케일 다운 지연 시간 (10분)
            # 새 노드 추가 후 안정화를 위한 시간
            - --scale-down-delay-after-add=10m

            # 노드 삭제 후 스케일 다운 지연 시간 (10분)
            # 삭제 작업 후 클러스터 안정화를 위한 시간
            - --scale-down-delay-after-delete=10m

            # 스케일 다운 실패 후 재시도 지연 시간 (3분)
            # 실패 후 재시도 전 대기 시간
            - --scale-down-delay-after-failure=3m

            # 불필요한 노드로 간주하기까지의 시간 (10분)
            # 노드가 지속적으로 저사용될 때 스케일 다운 고려 시간
            - --scale-down-unneeded-time=10m

            # 준비되지 않은 노드 스케일 다운까지의 시간 (20분)
            # 문제 있는 노드 처리를 위한 대기 시간
            - --scale-down-unready-time=20m

            # 스케일 다운 결정을 위한 리소스 사용률 임계값 (50%)
            - --scale-down-utilization-threshold=0.5

            # 스케일 다운 후보 노드 수 (3)
            # 전체 노드의 60%로 제한
            - --scale-down-non-empty-candidates-count=3

            # 스케일 다운 후보 풀 비율 (0.1 = 10%)
            - --scale-down-candidates-pool-ratio=0.1

            # 스케일 다운 후보 풀 최소 개수 (2)
            # 전체 노드의 40%로 제한
            - --scale-down-candidates-pool-min-count=2

            # 클러스터 상태 스캔 간격 (1분)
            # 자주 스캔하면 부하가 증가하므로 적절히 조절
            - --scan-interval=1m

            # 최대 총 노드 수 (5)
            # 클러스터의 최대 크기를 5개 노드로 제한
            - --max-nodes-total=5

            # 총 코어 수 제한 (min:max)
            # 5개 노드 * 2 core = 10 core
            - --cores-total=0:10

            # 총 메모리 제한 (min:max, GB)
            # 5개 노드 * 16GB = 80GB
            - --memory-total=0:80

            # 한 번에 삭제할 수 있는 최대 빈 노드 수 (2)
            # 전체 노드의 40%로 제한
            - --max-empty-bulk-delete=2

            # 노드 종료 전 최대 대기 시간 (10분)
            # 파드 정상 종료를 위한 충분한 시간 제공
            - --max-graceful-termination-sec=600

            # 허용 가능한 최대 준비되지 않은 노드 비율 (40%)
            # 소규모 클러스터에서는 더 높은 비율 허용
            - --max-total-unready-percentage=40

            # 허용 가능한 준비되지 않은 노드 수 (1)
            # 전체 노드의 20%로 제한
            - --ok-total-unready-count=1

            # 노드 그룹별 메트릭 생성 여부
            - --emit-per-nodegroup-metrics=true

            # 리소스 추정기 (binpacking: 리소스를 최대한 활용)
            - --estimator=binpacking

            # 노드 그룹 확장 전략 (random: 무작위 선택)
            - --expander=random

            # DaemonSet 파드의 리소스 사용량 무시 여부
            - --ignore-daemonsets-utilization=true

            # 미러 파드의 리소스 사용량 무시 여부
            - --ignore-mirror-pods-utilization=true

            # 상태 ConfigMap 작성 여부
            - --write-status-configmap=true

            # 상태 ConfigMap 이름
            - --status-config-map-name=cluster-autoscaler-status

            # 최대 비활성 시간 (15분)
            # 오토스케일러가 비활성 상태로 유지될 수 있는 최대 시간
            - --max-inactivity=15m

            # 최대 연속 실패 시간 (15분)
            # 연속 실패 후 오토스케일러가 비활성화되는 시간
            - --max-failing-time=15m

            # 제거 불가능한 노드 재확인 간격 (5분)
            - --unremovable-node-recheck-timeout=5m

            # 자동 프로비저닝된 노드 그룹의 최대 수 (1)
            # 소규모 클러스터에서는 노드 그룹 수를 최소화
            - --max-autoprovisioned-node-group-count=1

            # 제거 가능한 파드의 우선순위 컷오프
            - --expendable-pods-priority-cutoff=-10

            # 빈 노드의 DaemonSet 파드 제거 허용
            - --daemonset-eviction-for-empty-nodes=true

            # 사용 중인 노드의 DaemonSet 파드 제거 허용
            - --daemonset-eviction-for-occupied-nodes=false

            # 노드 종료 전 cordon 설정
            - --cordon-node-before-terminating=true

            # 중복 이벤트 기록 여부
            - --record-duplicated-events=false

            # 새 파드에 대한 스케일 업 지연 시간 (2분)
            # 불필요한 스케일 업을 방지하기 위한 대기 시간
            - --new-pod-scale-up-delay=2m

            # 동시에 처리할 수 있는 최대 스케일 다운 작업 수 (2)
            # 전체 노드의 40%로 제한
            - --max-scale-down-parallelism=2

            # soft taint를 한 번에 적용할 수 있는 최대 노드 수 (2)
            # 전체 노드의 40%로 제한
            - --max-bulk-soft-taint-count=2

            # 파드 제거 최대 대기 시간 (2분)
            - --max-pod-eviction-time=2m

            # 디버깅 스냅샷 활성화 여부
            - --debugging-snapshot-enabled=true

            # 노드 그룹 최소 크기 강제 적용 여부
            - --enforce-node-group-min-size=true

            # 시스템 파드가 있는 노드 스케일 다운 건너뛰기 여부
            - --skip-nodes-with-system-pods=true

            # 로컬 스토리지가 있는 노드 스케일 다운 건너뛰기 여부
            - --skip-nodes-with-local-storage=true

            # 최소 레플리카 수 (0)
            - --min-replica-count=0

            # 커스텀 컨트롤러 파드가 있는 노드 스케일 다운 건너뛰기 여부
            - --skip-nodes-with-custom-controller-pods=true

            # 유사한 노드 그룹 밸런싱 활성화
            - --balance-similar-node-groups=true

            # 노드 그룹 밸런싱 시 무시할 레이블들
            - --balancing-ignore-label=displayName
            - --balancing-ignore-label=hostname
            - --balancing-ignore-label=internal_addr
            - --balancing-ignore-label=oci.oraclecloud.com/fault-domain

          env:
            - name: OKE_USE_INSTANCE_PRINCIPAL
              value: "true"
            - name: OCI_SDK_APPEND_USER_AGENT
              value: "oci-oke-cluster-autoscaler"
# bbrick_oci/otel-collector/loki-values.yaml
mode: deployment
nameOverride: "otel-collector-loki"
fullnameOverride: "otel-collector-loki"

image:
  repository: docker.io/otel/opentelemetry-collector-contrib
  pullPolicy: IfNotPresent
  tag: "0.118.0"
  digest: ""

resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 200m
    memory: 400Mi

useGOMEMLIMIT: true

ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    protocol: TCP
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    protocol: TCP
  metrics:
    enabled: true
    containerPort: 8888
    servicePort: 8888
    protocol: TCP
  healthcheck:  # health_check를 healthcheck로 변경
    enabled: true
    containerPort: 13133
    servicePort: 13133
    protocol: TCP

command:
  name: otelcol-contrib

service:
  enabled: true
  type: ClusterIP
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8888"

clusterRole:
  create: true
  rules:
    - apiGroups: [""]
      resources: ["pods", "nodes", "namespaces"]
      verbs: ["get", "list", "watch"]
    - apiGroups: [""]
      resources: ["nodes/metrics", "nodes/stats"]
      verbs: ["get", "list", "watch"]

serviceAccount:
  create: true

config:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
    loki:
      protocols:
        http:
          endpoint: 0.0.0.0:3100

  processors:
    attributes:
      actions:
        - key: trace_id
          from_attribute: trace_id
          action: upsert
        - key: span_id
          from_attribute: span_id
          action: upsert
        - key: trace_flags
          from_attribute: trace_flags
          action: upsert

    batch:
      send_batch_size: 10000
      timeout: 10s
      send_batch_max_size: 20000
    
    memory_limiter:
      check_interval: 5s
      limit_percentage: 80
      spike_limit_percentage: 25
    
    k8sattributes:
      auth_type: serviceAccount
      passthrough: false
      extract:
        metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.deployment.name
          - k8s.namespace.name
          - k8s.node.name
          - k8s.container.name
    
    resource:
      attributes:
        - action: insert
          key: loki.format
          value: raw
        - action: insert
          key: service.name
          from_attribute: k8s.deployment.name

  exporters:
    loki:
      endpoint: "http://goyo-loki-distributed-gateway.monitoring.svc.cluster.local:80/loki/api/v1/push"
      default_labels_enabled:
        exporter: true
        job: true
      tls:
        insecure: true
      headers:
        x-scope-orgid: "goyo-svc"
      retry_on_failure:
        enabled: true
        initial_interval: 5s
        max_interval: 30s
        max_elapsed_time: 300s
      sending_queue:
        enabled: true
        num_consumers: 10
        queue_size: 5000
    
    debug:  # logging을 debug로 변경
      verbosity: detailed

  service:
    telemetry:
      metrics:
        address: 0.0.0.0:8888
        level: detailed
      logs:
        level: debug
        development: true
    
    extensions: [health_check]
    
    pipelines:
      logs:
        receivers: [otlp, loki]
        processors: [k8sattributes, resource, attributes, memory_limiter, batch]
        exporters: [loki, debug]

tolerations:
  - key: "goyo-svc"
    operator: "Equal"
    value: "web-apps"
    effect: "NoSchedule"

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: "goyo-svc"
          operator: In
          values:
          - "web-apps"

serviceMonitor:
  enabled: true

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8888"